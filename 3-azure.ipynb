{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once per environment\n",
    "%cd /notebooks/cerulean-notebook\n",
    "%pip install -r requirements.txt\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Set up environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set flags\n",
    "DEBUG = False\n",
    "\n",
    "data_dir = %env DATA_DIR\n",
    "azure_key = %env AZURE_KEY\n",
    "azure_endpoint = %env AZURE_ENDPOINT\n",
    "\n",
    "# Load SQL extension\n",
    "%load_ext sql\n",
    "%config SqlMagic.autopandas = True\n",
    "%config SqlMagic.feedback = False\n",
    "%config SqlMagic.displaycon = False\n",
    "\n",
    "\n",
    "# Have DuckDB use in-memory storage (comment out to user $DATABASE_URL file)\n",
    "%sql duckdb:///:memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from parquet\n",
    "\n",
    "# Load schema metadata from ${data_dir}physionet_schema.csv\n",
    "tables = pd.read_csv(f'{data_dir}/physionet_schema.csv', delimiter='\\t', usecols=['schema', 'table']).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Filter to mimiciv_note schema for now\n",
    "tables = tables[tables['schema'] == 'mimiciv_note']\n",
    "\n",
    "# Load data from parquet\n",
    "for (schema, table) in tables.values:\n",
    "    print(f'Loading {schema}.{table}')\n",
    "    %sql DROP TABLE IF EXISTS {{schema}}.{{table}}\n",
    "    %sql CREATE SCHEMA IF NOT EXISTS {{schema}}\n",
    "    %sql CREATE TABLE {{schema}}.{{table}} AS SELECT * FROM read_parquet('{{data_dir}}/parquet/{{schema}}/{{table}}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for client auth using key and endpoint\n",
    "def azure_auth_client ():\n",
    "    ta_credential = AzureKeyCredential (azure_key)\n",
    "    text_analytics_client = TextAnalyticsClient (\n",
    "        endpoint=azure_endpoint,\n",
    "        credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "# Authenticate the client\n",
    "azure_client = azure_auth_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for running Azure NLP for healthcare\n",
    "def azure_health(client, documents):\n",
    "    poller = client.begin_analyze_healthcare_entities(documents)\n",
    "    result = poller.result()\n",
    "    \n",
    "    # Translate result to dataframe of entities\n",
    "    \n",
    "    # results_df = pd.DataFrame(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random sample of 10 patient notes\n",
    "sample = %sql SELECT text, subject_id, charttime FROM mimiciv_note.discharge USING SAMPLE 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "documents = sample['text'].tolist()\n",
    "\n",
    "if DEBUG:\n",
    "    documents = [\n",
    "        \"Patient has a history of hypertension and type 2 diabetes. He is currently taking metformin and lisinopril.\"\n",
    "    ]\n",
    "\n",
    "# results = azure_health(azure_client, documents)\n",
    "# Save the results to a dataframe\n",
    "results = azure_health(azure_client, documents)\n",
    "sample['az_result'] = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results to a file\n",
    "if DEBUG:\n",
    "    print(sample['az_result'][0], file=open(\"debug/result_0.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sample['az_result'][0].entities[0].data_sources\n",
    "# display(ds)\n",
    "\n",
    "# make a dict with the data sources mapping name to entity id\n",
    "ds_dict = {d.name: d.entity_id for d in ds}['UMLS']\n",
    "display(ds_dict)\n",
    "\n",
    "# ds_umls = [d for d in ds if d.entity_id.startswith('UMLS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column sample['entity'] with the normalized text and category from each entity in sample['az_result']\n",
    "sample['entity'] = sample['az_result'].apply(lambda x: [(e.normalized_text, e.category) for e in x.entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
